Automated Essay Scoring (AES) is a field that has
become most popular in recent years due to the promises it holds
for the increased efficiencies and arms in terms of educational
assessments. This paper presents a comprehensive approach of
how to create an AES system using modern large language models
like LLaMA 3.2 1B tuned for grading from 1 to 6 for student
essays. Three different approaches have been taken to implement
it: a baseline with an off-the-shelf pre-trained language model,
a fine-tuned model trained on a labeled dataset, and a fine- tuned
model plus prompt engineering for optimized performance. The
results indicate that fine-tuning with prompt engineering
produced the highest accuracy with significant improvements in
validation performance. Other issues discussed in this paper are
overfitting, memory limitations, and the significance of prompt
design in LLM-based AES. The conclusion indicates that it could
be an efficient and scalable means of automatic grading of essays
in educational contexts without being an interference on the
grounds of fairness.
